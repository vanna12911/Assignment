{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS 1:\n",
    "\n",
    "Web scraping refers to the automated extraction of data from websites. It is a technique used to gather information from web pages by sending requests to web servers, receiving the HTML content, and then parsing and extracting the desired data from the page structure.\n",
    "\n",
    "Web scraping is used for several reasons across various industries \n",
    "1. Data Extraction\n",
    "2. Price monitoring\n",
    "3. Sentiment analysis and social media marketing\n",
    "3. Research and academic purposes\n",
    "4. Real estate and travel bookings\n",
    "\n",
    "Web Scraping is used in various areas to extract data from websites. Three common areas where Web Scraping is used are:\n",
    "\n",
    "1. E-commerce: Web Scraping is used to collect product information, prices, and reviews from various e-commerce websites. This data is then used to analyze the market trends and make informed business decisions.\n",
    "\n",
    "2. Social Media: Web Scraping is used to collect data from social media platforms like Facebook, Twitter, and LinkedIn. This data is used to analyze user behavior, sentiment analysis, and to understand the impact of social media on businesses.\n",
    "\n",
    "3. Research: Web Scraping is used in research to collect data from various sources like news websites, scientific articles, and academic journals. This data is then used for analysis and to draw insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS 2:\n",
    "There are several methods and techniques used for web scraping. The choice of method depends on factors such as the complexity of the website, the desired data to be extracted, and the programming languages or tools being used.\n",
    "\n",
    "1. Manual Extraction: This method involves manually copying and pasting data from web pages into a spreadsheet or another format. It is a straightforward approach for extracting small amounts of data but is time-consuming and not suitable for large-scale scraping.\n",
    "\n",
    "2. HTML Parsing: HTML parsing libraries, such as Beautiful Soup (Python), allow you to parse and navigate through the HTML structure of a web page. It enables you to extract specific elements, attributes, or text based on their tags, classes, or other identifiers.\n",
    "\n",
    "3. API Access: Some websites provide Application Programming Interfaces (APIs) that allow developers to access and retrieve data in a structured format. Instead of scraping the website directly, you can make requests to the API endpoints to obtain the desired data. APIs are often more reliable and efficient for data extraction if they are available.\n",
    "\n",
    "4. Web Scraping Libraries and Frameworks: There are several programming language-specific libraries and frameworks designed specifically for web scraping. For example, in Python, popular libraries include Scrapy, Requests-HTML, and Selenium. These libraries provide higher-level abstractions and tools for web scraping, making it easier to handle tasks like sending requests, handling cookies, handling JavaScript-rendered pages, and more.\n",
    "\n",
    "5. XPath and CSS Selectors: XPath and CSS selectors are query languages used to navigate and extract elements from an HTML document. They provide a more precise and flexible way to locate and extract data. Libraries like lxml (Python) and Selenium (Python, JavaScript) support XPath and CSS selector functionality.\n",
    "\n",
    "6. Regular Expressions (Regex): Regular expressions are powerful patterns that can be used to match and extract specific data from HTML or text. This method is useful when the data follows a consistent pattern, such as extracting email addresses, phone numbers, or specific tags from HTML.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS 3:\n",
    "\n",
    "Beautiful Soup is a popular Python library used for web scraping and parsing HTML or XML documents. It provides a convenient and efficient way to extract data from web pages by traversing the HTML structure.\n",
    "\n",
    "It offers several key features that make it widely used for web scraping:\n",
    "\n",
    "1. HTML Parsing: Beautiful Soup helps parse and navigate through the HTML structure of a web page. It handles malformed or messy HTML and provides a consistent interface to access and manipulate elements, attributes, and text within the document.\n",
    "\n",
    "2. Tag Searching: Beautiful Soup allows you to search for specific HTML tags based on their names, attributes, or CSS classes. This makes it easy to locate and extract desired data from the HTML document.\n",
    "\n",
    "3. Data Extraction: Once you've located specific HTML elements using Beautiful Soup's searching capabilities, you can extract data from them. This includes retrieving text, attribute values, or even the HTML structure itself.\n",
    "\n",
    "4. Navigational Capabilities: Beautiful Soup provides navigation methods to move through the HTML tree structure, such as moving up and down the hierarchy, navigating siblings, and accessing parent and child elements. This makes it flexible for extracting data from complex web page layouts.\n",
    "\n",
    "5. Robust Handling of HTML: Beautiful Soup handles common HTML issues, such as mismatched tags, unclosed tags, or other parsing errors. It can often make sense of messy HTML and still extract the desired data.\n",
    "\n",
    "6. Integration with Request Libraries: Beautiful Soup works well with popular HTTP request libraries in Python, such as Requests, allowing you to combine the power of both libraries for fetching and parsing web pages.\n",
    "\n",
    "7. Support for Python Versions: Beautiful Soup is compatible with both Python 2 and Python 3, making it accessible to a wide range of developers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS 4:\n",
    "Flask is a lightweight web framework that allows developers to quickly and easily build web applications. In this web scraping project, Flask is used for :\n",
    " \n",
    "1. Buildling Web Interfaces:\n",
    "\n",
    "    Flask allows to create a user-friendly web interface for our web scraping project.  Flask provides routing capabilities to handle different URL endpoints and render HTML templates to display the results.\n",
    "\n",
    "2. Building RESTful APIs:\n",
    "\n",
    "    Flask is used to develop APIs that expose the scraped data to other applications or services. we can even create RESTful APIs using Flask's routing capabilities, allowing other developers to access and consume the scraped data programmatically.\n",
    "\n",
    "3. Intergration with Database system: \n",
    "\n",
    "     Flask integrates well with various database systems, such as MySQL, PostgreSQL, or MongoDB. we used Flask's database integration capabilities to store and manage the scraped data in a structured and persistent manner.\n",
    "\n",
    "4. Scalability and deployment purposes:\n",
    "\n",
    "    Flask is lightweight and easy to deploy, making it suitable for web scraping projects. We deployed Flask applications on various platforms, including cloud services like azure and  AWS, allowing us to scale our application based on our scraping task.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS 5:\n",
    "In this project , we have used AWS services like <u>Code pipeline</u> and <u>Bean stalk</u> to deploy our code on AWS.\n",
    "\n",
    "\n",
    "1.<b>Code pipeline:</b>\n",
    "This is a service that helps us in automating the build, test and deployment of applications into production\n",
    "or other environments such as development, testing etc . It also provides continuous integration (CI) functionality which\n",
    "means it automatically builds your application every time you push changes from Git repository or any source control system(\n",
    "SCM).\n",
    "\n",
    "\n",
    "2.<b>Bean stalk:</b>\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment from capacity provisioning, load balancing, and auto scaling to application health monitoring.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
